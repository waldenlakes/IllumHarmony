<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SIDNet: Learning Shading-aware Illumination Descriptor for Image Harmonization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SIDNet: Learning Shading-aware Illumination Descriptor for Image Harmonization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=lNgMiXUAAAAJ" target="_blank">Zhongyun Hu</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=s1YzWUEAAAAJ" target="_blank">Ntumba Elie Nsampi</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=nR72f8QAAAAJ" target="_blank">Xue Wang</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=8D_9JI0AAAAJ" target="_blank">Qing Wang</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">School of Computer Science, Northwestern Polytechnical University<br>IEEE TETCI 2023</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/SIDNet_TETCI2023_High-Resolution.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (High-res)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Dataset Link. -->
                  <span class="link-block">
                    <a href="https://github.com/waldenlakes/IllumHarmony-Dataset" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                          <svg class="svg-inline--fa fa-images fa-w-18" aria-hidden="true" focusable="false" data-prefix="far" data-icon="images" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M480 416v16c0 26.51-21.49 48-48 48H48c-26.51 0-48-21.49-48-48V176c0-26.51 21.49-48 48-48h16v48H54a6 6 0 0 0-6 6v244a6 6 0 0 0 6 6h372a6 6 0 0 0 6-6v-10h48zm42-336H150a6 6 0 0 0-6 6v244a6 6 0 0 0 6 6h372a6 6 0 0 0 6-6V86a6 6 0 0 0-6-6zm6-48c26.51 0 48 21.49 48 48v256c0 26.51-21.49 48-48 48H144c-26.51 0-48-21.49-48-48V80c0-26.51 21.49-48 48-48h384zM264 144c0 22.091-17.909 40-40 40s-40-17.909-40-40 17.909-40 40-40 40 17.909 40 40zm-72 96l39.515-39.515c4.686-4.686 12.284-4.686 16.971 0L288 240l103.515-103.515c4.686-4.686 12.284-4.686 16.971 0L480 208v80H192v-48z"></path></svg><!-- <i class="far fa-images"></i> Font Awesome fontawesome.com -->
                      </span>
                      <span>Dataset</span>
                      </a>
                  </span></div>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/waldenlakes/SIDNet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2112.01314" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <strong>SIDNet:</strong> Given a foreground image (a) and a background image (b) where the foreground image is to be placed, our model first learns to extract <em>a shading-aware illumination descriptor</em> (c) from the background image, and <em>a set of shading bases</em> (d) and <em>albedo feature</em> (e) from the foreground image. Then, the shading-aware illumination descriptor and shading bases are combined to render a new foreground shading (f) by a rendering equation (g). Last, the albedo feature and the foreground shading are used to render a new foreground image (h) that conforms to the illumination of the background image. Our model is designed with the physical principle of image formation.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Image harmonization aims at adjusting the appearance of the foreground to make it more compatible with the background. Without exploring background illumination and its effects on the foreground elements, existing works are incapable of generating a realistic foreground shading. In this paper, we decompose the image harmonization task into two sub-problems: 1) illumination estimation of the background image and 2) re-rendering of foreground objects under background illumination. Before solving these two sub-problems, we first learn a shading-aware illumination descriptor via a well-designed neural rendering framework, of which the key is a shading bases module that generates multiple shading bases from the foreground image. Then we design a background illumination estimation module to extract the illumination descriptor from the background. Finally, the Shading-aware Illumination Descriptor is used in conjunction with the neural rendering framework (SIDNet) to produce the harmonized foreground image containing a novel harmonized shading. Moreover, we construct a large-scale photo-realistic synthetic image harmonization dataset (IllumHarmony-Dataset) that contains numerous shading variations. Extensive experiments on both synthetic and real data demonstrate the superiority of the proposed method, especially in dealing with foreground shadings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">IllumHarmony-Dataset</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/dataset_construction_pipeline.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The pipeline of constructing the pair of harmonized image and unharmonized image in the IllumHarmony-Dataset. It mainly covers data collection, rendering, and object placement. To train the SIDNet, shading and albedo are also rendered.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/illum_maps.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The t-SNE visualization of our collected illumination maps, which contain diverse weather conditions, illuminant colors, time of the day, and locations.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/dataset_example.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          High-quality photo-realistic examples from our constructed dataset. Red and green insets in the bottom row indicate that our dataset contains challenging shading variations.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/dataset_example2.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        More examples from our constructed dataset.
     </h2>
   </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">SIDNet</h2>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/sidnet.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          An overview of our proposed image harmonization method. Our method has two training stages: training the Neural Rendering Framework (NRF) and training the Background Illumination Estimation Module (BIEM). The key to the first stage is to learn a shading-aware illumination descriptor, which is then estimated from the background image in the second stage. During inference, our image harmonization pipeline (i.e., SIDNet) combines partial modules of the NRF {f,h,r} and the BIEM q to adjust the foreground appearance using the estimated background illumination descriptor.
       </h2>
      </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Our Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/user_study_results.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The results of SIDNet on real data across different weather conditions. Our intermediate shading results show that our SIDNet is capable of generating plausible shadings that are consistent with the background illumination.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/non_humans_results.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Generalization to non-human objects.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/indoor_scenes_results.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Generalization to indoor scenes.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Limitations</h2>
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/limitations.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          We suggest that future work tackle the current challenges that SIDNet faces: (1) distortion and loss of texture details in albedo estimation module. (2) The lack of spatially-varying harmonization in indoor scenes.
       </h2>
      </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hu2023sidnet,
        title={{SIDNet}: Learning Shading-aware Illumination Descriptor for Image Harmonization},
        author={Hu, Zhongyun and Nsampi, Ntumba Elie and Wang, Xue and Wang, Qing},
        journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
        year={2023}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
